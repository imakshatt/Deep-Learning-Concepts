{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Language Model Practical 7"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-11T05:53:33.075838Z","iopub.status.busy":"2024-04-11T05:53:33.075429Z","iopub.status.idle":"2024-04-11T05:53:34.550965Z","shell.execute_reply":"2024-04-11T05:53:34.549742Z","shell.execute_reply.started":"2024-04-11T05:53:33.075801Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T14:13:58.238161Z","iopub.status.busy":"2024-04-11T14:13:58.237271Z","iopub.status.idle":"2024-04-11T14:14:10.227661Z","shell.execute_reply":"2024-04-11T14:14:10.226575Z","shell.execute_reply.started":"2024-04-11T14:13:58.238118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras.preprocessing in /opt/conda/lib/python3.10/site-packages (1.1.2)\n","Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from keras.preprocessing) (1.26.4)\n","Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from keras.preprocessing) (1.16.0)\n"]}],"source":["!pip install keras.preprocessing"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T14:22:23.129243Z","iopub.status.busy":"2024-04-11T14:22:23.128889Z","iopub.status.idle":"2024-04-11T14:23:46.235087Z","shell.execute_reply":"2024-04-11T14:23:46.234151Z","shell.execute_reply.started":"2024-04-11T14:22:23.129219Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Data:  Jack and Jill went up the hill .\n"," To fetch a pail of water .\n"," Jack fell down and broke his crown .\n"," And Jill came tumbling after . <class 'str'>\n","Data_Splitted: [' Jack and Jill went up the hill .', ' To fetch a pail of water .', ' Jack fell down and broke his crown .', ' And Jill came tumbling after .'] <class 'list'>\n","Word Indices: {'.': 1, 'and': 2, 'jack': 3, 'jill': 4, 'went': 5, 'up': 6, 'the': 7, 'hill': 8, 'to': 9, 'fetch': 10, 'a': 11, 'pail': 12, 'of': 13, 'water': 14, 'fell': 15, 'down': 16, 'broke': 17, 'his': 18, 'crown': 19, 'came': 20, 'tumbling': 21, 'after': 22}\n","Vocab Size: 23\n","Sequences: [[3, 2, 4, 5, 6, 7, 8, 1], [9, 10, 11, 12, 13, 14, 1], [3, 15, 16, 2, 17, 18, 19, 1], [2, 4, 20, 21, 22, 1]] <class 'list'> 4\n","X= [[3, 2, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14], [3, 15, 16, 2, 17, 18, 19], [2, 4, 20, 21, 22]] y= [[3, 2, 4, 5, 6, 7, 8, 1], [9, 10, 11, 12, 13, 14, 1], [3, 15, 16, 2, 17, 18, 19, 1], [2, 4, 20, 21, 22, 1]] <class 'list'> <class 'list'>\n","Maxlen: 7\n","X: [[ 0  3  2  4  5  6  7  8]\n"," [ 0  0  9 10 11 12 13 14]\n"," [ 0  3 15 16  2 17 18 19]\n"," [ 0  0  0  2  4 20 21 22]] <class 'numpy.ndarray'> (4, 8)\n","y: [[ 3  2  4  5  6  7  8  1]\n"," [ 0  9 10 11 12 13 14  1]\n"," [ 3 15 16  2 17 18 19  1]\n"," [ 0  0  2  4 20 21 22  1]] <class 'numpy.ndarray'> (4, 8)\n","X= [[ 0  3  2  4  5  6  7  8]\n"," [ 0  0  9 10 11 12 13 14]\n"," [ 0  3 15 16  2 17 18 19]\n"," [ 0  0  0  2  4 20 21 22]] y= [[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n","  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n","  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]] <class 'numpy.ndarray'> <class 'numpy.ndarray'> (4, 8) (4, 8, 23)\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_4\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1712845347.080114     102 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","W0000 00:00:1712845347.103169     102 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["-------------------Probability of Input Sentence------------------------------\n","Input Sentence: Jack and Jill Went\n","Encoded: [[0 3 2 4 5]] (1, 5)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [1.07511999e-02 5.16224129e-04 4.42457914e-01 6.31036062e-04\n","   1.32536748e-03 3.52948811e-03 2.21741066e-04 1.10185018e-03\n","   4.59189177e-04 4.05240357e-02 9.81476158e-03 1.09608727e-05\n","   1.68666200e-04 2.49970355e-04 3.26088717e-04 4.79244679e-01\n","   1.93832879e-04 4.47920663e-03 1.00518424e-04 5.49756223e-04\n","   2.93796277e-03 3.97367839e-04 8.13550832e-06]\n","  [1.84240111e-04 1.76487774e-05 4.35098493e-03 9.80140758e-04\n","   8.41526568e-01 1.58325842e-04 8.72439356e-04 4.12817244e-05\n","   8.56123006e-05 7.00588207e-05 3.08749429e-03 3.74098448e-03\n","   3.79745620e-06 1.34840566e-05 6.06472837e-04 4.34905196e-05\n","   1.40899673e-01 8.31829038e-06 1.66457705e-03 7.44952704e-05\n","   3.64844134e-04 8.62069894e-04 3.42960848e-04]\n","  [1.79449751e-04 3.16888909e-04 2.99849603e-02 4.02460391e-05\n","   1.52750209e-03 9.47400391e-01 1.20480210e-04 3.87386594e-04\n","   5.68454561e-04 4.49299841e-04 9.10355127e-04 6.67199143e-04\n","   9.30340728e-04 7.99043210e-06 1.50818223e-05 5.01776580e-03\n","   1.19300028e-04 1.09169388e-03 4.33195646e-06 9.71999718e-04\n","   9.01944656e-03 2.11071776e-04 5.84418267e-05]\n","  [2.36627147e-05 3.99285673e-05 2.09600956e-04 8.51021687e-05\n","   7.52725609e-05 2.98692867e-05 9.79694843e-01 1.37687603e-05\n","   6.03691587e-05 9.69635585e-05 2.16258832e-05 4.46503283e-04\n","   2.83053331e-03 7.25278456e-04 9.49531795e-07 2.11244151e-05\n","   9.31498886e-04 7.00743822e-03 1.68972847e-03 2.23713482e-06\n","   4.21031285e-03 1.47438224e-03 3.09141818e-04]]] (1, 5, 23)\n","Probability of Sentence \" Jack and Jill Went \" is: 0.1684744944389609\n","Input Sentence: and Jill Went up\n","Encoded: [[0 2 4 5 6]] (1, 5)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [2.29452595e-01 7.71042425e-04 2.07244128e-01 6.21089479e-03\n","   2.75788549e-03 3.76727176e-03 3.30060546e-04 1.07133924e-03\n","   1.58382900e-04 3.37981194e-01 3.07156728e-03 1.56293936e-05\n","   3.21527448e-04 4.55196743e-04 3.57997604e-04 1.94590300e-01\n","   1.15915907e-04 7.43924081e-03 2.27148092e-04 1.64792477e-03\n","   1.65000628e-03 3.45094537e-04 1.75051609e-05]\n","  [1.44180201e-04 2.97200859e-05 2.50178203e-02 1.56155467e-04\n","   6.66070953e-02 1.29182159e-03 1.30637013e-03 3.73309711e-04\n","   2.49127112e-03 2.46427400e-04 7.32317507e-01 2.45649461e-03\n","   3.38492755e-05 6.31656585e-05 6.14955323e-04 5.15364157e-03\n","   1.48091272e-01 2.18000845e-04 2.06731772e-03 4.69574559e-04\n","   6.65964140e-03 3.77324386e-03 4.17109113e-04]\n","  [3.13436525e-04 2.78986321e-04 2.17190161e-02 2.28001247e-03\n","   5.02032377e-02 5.49300620e-03 1.13165355e-03 6.47169829e-04\n","   1.93434404e-04 6.01138418e-05 4.46420658e-04 8.97390127e-01\n","   1.21062191e-03 2.02208448e-05 2.17411158e-04 5.76367420e-05\n","   9.81004070e-03 2.80284385e-05 1.19246461e-03 1.58582407e-03\n","   1.53099443e-03 1.93317584e-03 2.25700997e-03]\n","  [1.83778160e-04 6.80119963e-04 1.65916237e-04 9.97954194e-05\n","   1.00357702e-05 3.77452257e-03 5.83003182e-03 1.17728568e-03\n","   2.68897013e-04 8.83484725e-04 2.07125995e-05 4.53575252e-04\n","   9.15634573e-01 4.58070356e-03 4.35346647e-06 1.26835098e-03\n","   9.46958880e-06 5.10719940e-02 8.19298439e-05 1.78454095e-04\n","   1.03311753e-02 9.33090632e-04 2.35773157e-03]]] (1, 5, 23)\n","Probability of Sentence \" and Jill Went up \" is: 1.2840351356553234e-11\n","Input Sentence: went up the hill\n","Encoded: [[0 5 6 7 8]] (1, 5)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [1.23865925e-01 8.26092146e-04 3.12865973e-01 4.22147755e-03\n","   1.94675080e-03 4.07853769e-03 3.56242555e-04 1.27297489e-03\n","   2.73223122e-04 2.59008318e-01 3.96643206e-03 1.30574508e-05\n","   3.38968355e-04 4.39820142e-04 3.65272776e-04 2.74333179e-01\n","   1.84309276e-04 7.58543005e-03 2.12339757e-04 1.22885336e-03\n","   2.27540708e-03 3.24833847e-04 1.66135869e-05]\n","  [1.96875044e-04 4.98315494e-05 4.89785336e-02 4.22639161e-04\n","   2.34610736e-01 1.09050958e-03 2.11307360e-03 5.72615128e-04\n","   2.49050045e-03 2.41901376e-04 3.44232291e-01 4.48727235e-03\n","   2.11115821e-05 6.39050922e-05 1.63185748e-03 1.80827407e-03\n","   3.44738871e-01 1.00241858e-04 3.54865682e-03 5.95366815e-04\n","   3.56658082e-03 3.84337618e-03 5.94949292e-04]\n","  [4.19915450e-04 1.51993358e-03 9.55639556e-02 1.26956089e-03\n","   4.09866571e-02 7.57706910e-02 2.06159800e-03 2.49174819e-03\n","   1.82575348e-03 1.32950721e-04 3.22105945e-03 7.39805877e-01\n","   2.91598006e-03 5.96649807e-05 6.47105044e-04 5.47189615e-04\n","   1.31972339e-02 1.00217403e-04 8.35580868e-04 5.83950616e-03\n","   3.73555766e-03 3.07245762e-03 3.97977885e-03]\n","  [1.36248927e-04 1.83631526e-03 1.28027852e-04 1.84716642e-04\n","   2.42976512e-05 1.53917330e-03 1.04810067e-01 7.67965568e-04\n","   5.88084513e-04 8.53847421e-04 2.07106241e-05 2.32762890e-03\n","   7.28111506e-01 1.58705600e-02 1.09898401e-05 4.31506371e-04\n","   5.21318398e-05 1.17074601e-01 9.02888190e-04 1.14045943e-04\n","   1.02825593e-02 4.27538343e-03 9.65666957e-03]]] (1, 5, 23)\n","Probability of Sentence \" went up the hill \" is: 6.216642724308498e-14\n","Input Sentence: jack and Jill Went up the hill .\n","Encoded: [[0 3 2 4 5 6 7 8 1]] (1, 9)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [1.07511999e-02 5.16224129e-04 4.42457914e-01 6.31036062e-04\n","   1.32536748e-03 3.52948811e-03 2.21741066e-04 1.10185018e-03\n","   4.59189177e-04 4.05240357e-02 9.81476158e-03 1.09608727e-05\n","   1.68666200e-04 2.49970355e-04 3.26088717e-04 4.79244679e-01\n","   1.93832879e-04 4.47920663e-03 1.00518424e-04 5.49756223e-04\n","   2.93796277e-03 3.97367839e-04 8.13550832e-06]\n","  [1.84240111e-04 1.76487774e-05 4.35098493e-03 9.80140758e-04\n","   8.41526568e-01 1.58325842e-04 8.72439356e-04 4.12817244e-05\n","   8.56123006e-05 7.00588207e-05 3.08749429e-03 3.74098448e-03\n","   3.79745620e-06 1.34840566e-05 6.06472837e-04 4.34905196e-05\n","   1.40899673e-01 8.31829038e-06 1.66457705e-03 7.44952704e-05\n","   3.64844134e-04 8.62069894e-04 3.42960848e-04]\n","  [1.79449751e-04 3.16888909e-04 2.99849603e-02 4.02460391e-05\n","   1.52750209e-03 9.47400391e-01 1.20480210e-04 3.87386594e-04\n","   5.68454561e-04 4.49299841e-04 9.10355127e-04 6.67199143e-04\n","   9.30340728e-04 7.99043210e-06 1.50818223e-05 5.01776580e-03\n","   1.19300028e-04 1.09169388e-03 4.33195646e-06 9.71999718e-04\n","   9.01944656e-03 2.11071776e-04 5.84418267e-05]\n","  [2.36627147e-05 3.99285673e-05 2.09600956e-04 8.51021687e-05\n","   7.52725609e-05 2.98692867e-05 9.79694843e-01 1.37687603e-05\n","   6.03691587e-05 9.69635585e-05 2.16258832e-05 4.46503283e-04\n","   2.83053331e-03 7.25278456e-04 9.49531795e-07 2.11244151e-05\n","   9.31498886e-04 7.00743822e-03 1.68972847e-03 2.23713482e-06\n","   4.21031285e-03 1.47438224e-03 3.09141818e-04]\n","  [3.62463499e-04 2.23863481e-06 9.26414912e-04 1.33566296e-04\n","   1.41510645e-05 1.92576554e-05 2.24927626e-05 9.84704137e-01\n","   3.08538401e-05 6.65072468e-04 8.81327869e-05 2.79115484e-05\n","   1.26640152e-04 3.19288392e-03 1.34577043e-03 3.09011462e-04\n","   1.80679694e-04 5.70844804e-06 4.11493238e-03 3.50536284e-04\n","   9.50347967e-05 3.17380042e-03 1.08282329e-04]\n","  [3.34981701e-06 6.17392070e-04 2.19124468e-05 1.63178684e-05\n","   5.11875296e-05 4.14003414e-04 6.27569534e-05 1.89085076e-05\n","   9.84677434e-01 7.54418625e-06 5.93659701e-04 1.56779832e-04\n","   1.16686851e-05 1.79410912e-04 2.63497722e-03 6.56417760e-05\n","   6.80871526e-05 1.75504727e-04 1.66309674e-05 5.62457973e-03\n","   1.85213285e-05 2.94890906e-05 4.53428039e-03]\n","  [1.96649300e-04 9.96697724e-01 1.65601232e-05 3.56546865e-04\n","   6.00901294e-05 3.97207114e-05 6.62825187e-05 1.02884478e-05\n","   7.61502815e-05 2.17992019e-05 1.83331588e-06 1.20077209e-04\n","   7.49684623e-05 4.66970450e-05 1.76587177e-03 9.00164378e-06\n","   1.42235367e-05 1.29420660e-04 9.02719767e-05 5.81213899e-05\n","   1.13178373e-06 1.14745546e-06 1.45285667e-04]\n","  [1.77940354e-01 3.42511296e-01 5.12525858e-03 7.38270134e-02\n","   4.96480025e-05 1.25504553e-03 1.12235388e-02 9.39487386e-03\n","   1.33834756e-03 1.32504821e-01 8.19893903e-04 1.87800746e-04\n","   3.74577902e-02 1.33332917e-02 1.62997004e-03 1.62768885e-02\n","   4.40932345e-04 1.65381059e-01 4.99104382e-03 2.63796630e-03\n","   1.44734920e-03 1.17441079e-04 1.08354994e-04]]] (1, 9, 23)\n","Probability of Sentence \" jack and Jill Went up the hill . \" is: 0.15951010395124232\n","Input Sentence: to fetch a pail\n","Encoded: [[ 0  9 10 11 12]] (1, 5)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [9.89265181e-03 7.70400802e-04 4.76352692e-01 5.10035665e-04\n","   1.63457356e-03 5.47168823e-03 1.98121692e-04 1.04653765e-03\n","   4.98031615e-04 3.43338177e-02 1.32534606e-02 1.63805598e-05\n","   1.76689136e-04 2.33768820e-04 4.30711108e-04 4.47010398e-01\n","   1.84811041e-04 3.92242288e-03 1.09606408e-04 7.30626518e-04\n","   2.88054044e-03 3.32768570e-04 9.26713255e-06]\n","  [5.71516830e-05 1.42425897e-05 3.89331649e-03 3.21606261e-04\n","   8.88207480e-02 2.60680972e-04 2.99920840e-03 1.25040315e-04\n","   4.08829306e-04 3.29121285e-05 1.06285848e-02 1.63810644e-02\n","   1.54331101e-05 2.09447971e-05 5.55084378e-04 5.73345096e-05\n","   8.66785228e-01 1.27727353e-05 3.08543164e-03 6.54877440e-05\n","   1.49890129e-03 3.05908290e-03 9.00932006e-04]\n","  [1.45505252e-03 3.70265683e-03 7.25556791e-01 3.70915979e-03\n","   5.31319901e-03 1.76728666e-01 1.75321021e-03 1.24311224e-02\n","   9.04204382e-04 1.79581030e-03 3.16945545e-04 3.24052721e-02\n","   8.20632093e-03 2.01280171e-04 6.69200963e-04 9.10471717e-04\n","   9.98722389e-04 7.21363933e-04 4.75067034e-04 1.45913623e-02\n","   3.60396248e-03 2.16432498e-03 1.38591055e-03]\n","  [1.39841868e-04 3.77935660e-03 9.00999730e-05 3.31165102e-05\n","   3.73405055e-05 8.56372702e-04 6.98836073e-02 2.77935353e-04\n","   5.44687361e-03 1.09750521e-03 1.82139440e-04 1.11776651e-04\n","   3.55424844e-02 1.88174397e-02 3.16416117e-05 2.68542208e-03\n","   4.26686820e-05 8.50779533e-01 6.87839522e-04 5.05051758e-05\n","   3.45806614e-03 1.17594865e-03 4.79246490e-03]]] (1, 5, 23)\n","Probability of Sentence \" to fetch a pail \" is: 2.8413036469002933e-08\n","Input Sentence: fetch a pail\n","Encoded: [[ 0 10 11 12]] (1, 4)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [3.52905504e-02 9.76158888e-04 4.43592250e-01 1.41980441e-03\n","   1.82899286e-03 5.41560072e-03 2.67972035e-04 1.19117368e-03\n","   4.45898855e-04 9.41080004e-02 8.66215583e-03 1.67951657e-05\n","   2.67871364e-04 3.30686482e-04 4.54819412e-04 3.95476013e-01\n","   2.09301259e-04 5.71723562e-03 1.66809201e-04 1.03992526e-03\n","   2.76787858e-03 3.40043742e-04 1.41560613e-05]\n","  [1.27839419e-04 3.44945147e-05 1.52693540e-02 5.25392767e-04\n","   1.65460512e-01 4.04375285e-04 3.66276619e-03 2.66973511e-04\n","   8.11127305e-04 9.52306073e-05 4.59330007e-02 1.05856964e-02\n","   2.20706806e-05 3.86951106e-05 1.13706535e-03 2.07787452e-04\n","   7.42819071e-01 3.32272248e-05 5.28609846e-03 1.93765460e-04\n","   2.27892119e-03 3.90267698e-03 9.03864566e-04]\n","  [1.59445568e-03 3.34604341e-03 5.10472596e-01 3.05641606e-03\n","   2.39023920e-02 2.87716538e-01 2.37582950e-03 8.22161231e-03\n","   1.46461232e-03 9.34041105e-04 1.29891047e-03 1.14187174e-01\n","   7.87689257e-03 1.60529074e-04 7.48069724e-04 1.58058770e-03\n","   2.30072206e-03 4.83138661e-04 5.28798089e-04 1.69156492e-02\n","   5.10628661e-03 3.49095627e-03 2.23784638e-03]]] (1, 4, 23)\n","Probability of Sentence \" fetch a pail \" is: 1.3188703465945818e-14\n","Input Sentence: to fetch a pail of water\n","Encoded: [[ 0  9 10 11 12 13 14]] (1, 7)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [9.89265181e-03 7.70400802e-04 4.76352692e-01 5.10035665e-04\n","   1.63457356e-03 5.47168823e-03 1.98121692e-04 1.04653765e-03\n","   4.98031615e-04 3.43338177e-02 1.32534606e-02 1.63805598e-05\n","   1.76689136e-04 2.33768820e-04 4.30711108e-04 4.47010398e-01\n","   1.84811041e-04 3.92242288e-03 1.09606408e-04 7.30626518e-04\n","   2.88054044e-03 3.32768570e-04 9.26713255e-06]\n","  [5.71516830e-05 1.42425897e-05 3.89331649e-03 3.21606261e-04\n","   8.88207480e-02 2.60680972e-04 2.99920840e-03 1.25040315e-04\n","   4.08829306e-04 3.29121285e-05 1.06285848e-02 1.63810644e-02\n","   1.54331101e-05 2.09447971e-05 5.55084378e-04 5.73345096e-05\n","   8.66785228e-01 1.27727353e-05 3.08543164e-03 6.54877440e-05\n","   1.49890129e-03 3.05908290e-03 9.00932006e-04]\n","  [1.45505252e-03 3.70265683e-03 7.25556791e-01 3.70915979e-03\n","   5.31319901e-03 1.76728666e-01 1.75321021e-03 1.24311224e-02\n","   9.04204382e-04 1.79581030e-03 3.16945545e-04 3.24052721e-02\n","   8.20632093e-03 2.01280171e-04 6.69200963e-04 9.10471717e-04\n","   9.98722389e-04 7.21363933e-04 4.75067034e-04 1.45913623e-02\n","   3.60396248e-03 2.16432498e-03 1.38591055e-03]\n","  [1.39841868e-04 3.77935660e-03 9.00999730e-05 3.31165102e-05\n","   3.73405055e-05 8.56372702e-04 6.98836073e-02 2.77935353e-04\n","   5.44687361e-03 1.09750521e-03 1.82139440e-04 1.11776651e-04\n","   3.55424844e-02 1.88174397e-02 3.16416117e-05 2.68542208e-03\n","   4.26686820e-05 8.50779533e-01 6.87839522e-04 5.05051758e-05\n","   3.45806614e-03 1.17594865e-03 4.79246490e-03]\n","  [1.47187419e-03 1.81906347e-04 2.51955818e-04 1.83482433e-03\n","   5.90179989e-05 4.91507399e-06 5.25511045e-04 1.82542488e-01\n","   4.79881390e-04 1.19142188e-03 1.76731299e-03 1.43144774e-04\n","   1.06454201e-04 4.30920869e-02 2.80668829e-02 8.54743412e-05\n","   5.31927962e-03 6.23887172e-05 7.27477312e-01 6.42490748e-04\n","   1.00254940e-04 3.02700582e-03 1.56611321e-03]\n","  [5.75337675e-04 1.55987907e-02 7.58905604e-04 2.19157571e-03\n","   2.30010541e-04 1.79096102e-03 1.25294564e-05 8.16395390e-04\n","   8.50951448e-02 2.51010351e-04 5.93292702e-04 3.28221533e-04\n","   1.17692747e-04 3.64052597e-04 6.52771369e-02 2.16125001e-04\n","   5.40684086e-05 1.72606073e-04 2.79507571e-04 8.17057550e-01\n","   8.32666865e-06 2.25709027e-05 8.18815734e-03]]] (1, 7, 23)\n","Probability of Sentence \" to fetch a pail of water \" is: 1.5006256424943553e-11\n","Input Sentence: jack fell down and\n","Encoded: [[ 0  3 15 16  2]] (1, 5)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [1.07511999e-02 5.16224129e-04 4.42457914e-01 6.31036062e-04\n","   1.32536748e-03 3.52948811e-03 2.21741066e-04 1.10185018e-03\n","   4.59189177e-04 4.05240357e-02 9.81476158e-03 1.09608727e-05\n","   1.68666200e-04 2.49970355e-04 3.26088717e-04 4.79244679e-01\n","   1.93832879e-04 4.47920663e-03 1.00518424e-04 5.49756223e-04\n","   2.93796277e-03 3.97367839e-04 8.13550832e-06]\n","  [5.31213336e-05 1.00160923e-05 3.76497954e-03 2.82069173e-04\n","   1.15509421e-01 2.32763559e-04 1.71506719e-03 1.38638148e-04\n","   3.63658590e-04 3.09981951e-05 9.82384942e-03 1.00613534e-02\n","   7.99069767e-06 1.83172142e-05 6.31286704e-04 6.04230336e-05\n","   8.50742221e-01 8.09623180e-06 2.82118679e-03 6.39336067e-05\n","   9.27107409e-04 2.08577095e-03 6.47623907e-04]\n","  [2.34519900e-03 3.12186498e-03 9.21305656e-01 1.08214812e-02\n","   4.82300064e-03 3.28954235e-02 5.46924479e-04 1.98931410e-03\n","   1.43035621e-04 1.79227896e-03 4.33527821e-05 9.58222710e-03\n","   1.75598392e-03 2.57073389e-05 2.14076179e-04 1.31861467e-04\n","   1.92677471e-04 2.39521105e-04 1.88950391e-04 6.39198534e-03\n","   9.32099007e-04 2.27602519e-04 2.89918127e-04]\n","  [6.69609813e-04 1.63692783e-03 1.53858971e-04 8.36755207e-05\n","   2.31293143e-05 5.94851736e-04 7.47965323e-03 3.83209954e-05\n","   5.58799191e-04 2.46469839e-03 1.09716690e-04 6.96239795e-06\n","   6.96358411e-03 1.32462487e-03 3.61345701e-06 3.63424327e-03\n","   5.53232621e-06 9.73398924e-01 8.15368621e-05 1.33706790e-05\n","   5.81509958e-04 3.47123714e-05 1.38214134e-04]]] (1, 5, 23)\n","Probability of Sentence \" jack fell down and \" is: 0.17939892099916682\n","Input Sentence: jack fell down and broke\n","Encoded: [[ 0  3 15 16  2 17]] (1, 6)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [1.07511999e-02 5.16224129e-04 4.42457914e-01 6.31036062e-04\n","   1.32536748e-03 3.52948811e-03 2.21741066e-04 1.10185018e-03\n","   4.59189177e-04 4.05240357e-02 9.81476158e-03 1.09608727e-05\n","   1.68666200e-04 2.49970355e-04 3.26088717e-04 4.79244679e-01\n","   1.93832879e-04 4.47920663e-03 1.00518424e-04 5.49756223e-04\n","   2.93796277e-03 3.97367839e-04 8.13550832e-06]\n","  [5.31213336e-05 1.00160923e-05 3.76497954e-03 2.82069173e-04\n","   1.15509421e-01 2.32763559e-04 1.71506719e-03 1.38638148e-04\n","   3.63658590e-04 3.09981951e-05 9.82384942e-03 1.00613534e-02\n","   7.99069767e-06 1.83172142e-05 6.31286704e-04 6.04230336e-05\n","   8.50742221e-01 8.09623180e-06 2.82118679e-03 6.39336067e-05\n","   9.27107409e-04 2.08577095e-03 6.47623907e-04]\n","  [2.34519900e-03 3.12186498e-03 9.21305656e-01 1.08214812e-02\n","   4.82300064e-03 3.28954235e-02 5.46924479e-04 1.98931410e-03\n","   1.43035621e-04 1.79227896e-03 4.33527821e-05 9.58222710e-03\n","   1.75598392e-03 2.57073389e-05 2.14076179e-04 1.31861467e-04\n","   1.92677471e-04 2.39521105e-04 1.88950391e-04 6.39198534e-03\n","   9.32099007e-04 2.27602519e-04 2.89918127e-04]\n","  [6.69609813e-04 1.63692783e-03 1.53858971e-04 8.36755207e-05\n","   2.31293143e-05 5.94851736e-04 7.47965323e-03 3.83209954e-05\n","   5.58799191e-04 2.46469839e-03 1.09716690e-04 6.96239795e-06\n","   6.96358411e-03 1.32462487e-03 3.61345701e-06 3.63424327e-03\n","   5.53232621e-06 9.73398924e-01 8.15368621e-05 1.33706790e-05\n","   5.81509958e-04 3.47123714e-05 1.38214134e-04]\n","  [3.98608128e-04 1.01313362e-05 4.35492868e-04 4.50071238e-04\n","   1.26891406e-04 1.70397561e-06 5.10562037e-04 5.23286406e-03\n","   3.36762423e-05 3.71264352e-04 1.41272973e-03 1.06257692e-04\n","   1.30659619e-05 2.61246623e-03 8.11679929e-04 2.85779588e-05\n","   3.15783918e-03 3.99046294e-05 9.82578814e-01 5.76308776e-05\n","   6.68113644e-05 1.33599283e-03 2.07072560e-04]]] (1, 6, 23)\n","Probability of Sentence \" jack fell down and broke \" is: 0.17462671664472804\n","Input Sentence: fell down and broke\n","Encoded: [[ 0 15 16  2 17]] (1, 5)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [3.19301821e-02 8.15549225e-04 4.42880303e-01 1.47772965e-03\n","   1.54456845e-03 4.25555045e-03 2.74557038e-04 1.36675860e-03\n","   4.49193147e-04 9.61277783e-02 8.39668047e-03 1.34197744e-05\n","   2.58013606e-04 3.55879398e-04 4.13013389e-04 3.98372620e-01\n","   2.49551405e-04 6.38743071e-03 1.75168214e-04 8.43238959e-04\n","   3.02968640e-03 3.69301531e-04 1.39303447e-05]\n","  [7.89108162e-04 4.98752597e-05 3.50092947e-02 3.10827326e-03\n","   6.37770176e-01 3.37707286e-04 2.37523392e-03 1.30491797e-04\n","   2.21424110e-04 2.42672468e-04 9.93894041e-03 7.72096822e-03\n","   1.63957320e-05 2.36803880e-05 8.50802287e-04 1.25509250e-04\n","   2.93151379e-01 2.97736715e-05 4.69224481e-03 2.39126850e-04\n","   1.34149718e-03 1.29838230e-03 5.37033018e-04]\n","  [5.35062514e-03 1.53256231e-03 5.58685362e-01 2.46356614e-03\n","   4.29144725e-02 3.51964980e-01 3.60730337e-04 6.96827890e-04\n","   2.21771610e-04 7.23248394e-03 8.76680890e-04 2.29562959e-03\n","   1.42173131e-03 1.98359103e-05 5.30685720e-05 6.05474273e-03\n","   1.30771397e-04 2.43420782e-03 3.23214590e-05 2.34436523e-03\n","   1.23319374e-02 5.30576915e-04 5.08331832e-05]\n","  [3.49256938e-04 2.07300298e-04 1.31282292e-03 1.76710848e-04\n","   8.43299669e-04 8.53187754e-04 6.19229496e-01 1.17291893e-04\n","   3.55755532e-04 2.97955656e-03 1.28054467e-03 6.76911732e-04\n","   1.74343288e-02 1.69103069e-03 2.61579203e-06 2.07941630e-03\n","   2.37580203e-03 2.00444847e-01 3.31333489e-03 1.72888940e-05\n","   1.35792077e-01 7.95963313e-03 5.07546589e-04]]] (1, 5, 23)\n","Probability of Sentence \" fell down and broke \" is: 2.028430494035784e-12\n","Input Sentence: jack fell down and broke his crown\n","Encoded: [[ 0  3 15 16  2 17 18 19]] (1, 8)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [1.07511999e-02 5.16224129e-04 4.42457914e-01 6.31036062e-04\n","   1.32536748e-03 3.52948811e-03 2.21741066e-04 1.10185018e-03\n","   4.59189177e-04 4.05240357e-02 9.81476158e-03 1.09608727e-05\n","   1.68666200e-04 2.49970355e-04 3.26088717e-04 4.79244679e-01\n","   1.93832879e-04 4.47920663e-03 1.00518424e-04 5.49756223e-04\n","   2.93796277e-03 3.97367839e-04 8.13550832e-06]\n","  [5.31213336e-05 1.00160923e-05 3.76497954e-03 2.82069173e-04\n","   1.15509421e-01 2.32763559e-04 1.71506719e-03 1.38638148e-04\n","   3.63658590e-04 3.09981951e-05 9.82384942e-03 1.00613534e-02\n","   7.99069767e-06 1.83172142e-05 6.31286704e-04 6.04230336e-05\n","   8.50742221e-01 8.09623180e-06 2.82118679e-03 6.39336067e-05\n","   9.27107409e-04 2.08577095e-03 6.47623907e-04]\n","  [2.34519900e-03 3.12186498e-03 9.21305656e-01 1.08214812e-02\n","   4.82300064e-03 3.28954235e-02 5.46924479e-04 1.98931410e-03\n","   1.43035621e-04 1.79227896e-03 4.33527821e-05 9.58222710e-03\n","   1.75598392e-03 2.57073389e-05 2.14076179e-04 1.31861467e-04\n","   1.92677471e-04 2.39521105e-04 1.88950391e-04 6.39198534e-03\n","   9.32099007e-04 2.27602519e-04 2.89918127e-04]\n","  [6.69609813e-04 1.63692783e-03 1.53858971e-04 8.36755207e-05\n","   2.31293143e-05 5.94851736e-04 7.47965323e-03 3.83209954e-05\n","   5.58799191e-04 2.46469839e-03 1.09716690e-04 6.96239795e-06\n","   6.96358411e-03 1.32462487e-03 3.61345701e-06 3.63424327e-03\n","   5.53232621e-06 9.73398924e-01 8.15368621e-05 1.33706790e-05\n","   5.81509958e-04 3.47123714e-05 1.38214134e-04]\n","  [3.98608128e-04 1.01313362e-05 4.35492868e-04 4.50071238e-04\n","   1.26891406e-04 1.70397561e-06 5.10562037e-04 5.23286406e-03\n","   3.36762423e-05 3.71264352e-04 1.41272973e-03 1.06257692e-04\n","   1.30659619e-05 2.61246623e-03 8.11679929e-04 2.85779588e-05\n","   3.15783918e-03 3.99046294e-05 9.82578814e-01 5.76308776e-05\n","   6.68113644e-05 1.33599283e-03 2.07072560e-04]\n","  [3.26463458e-04 3.60732200e-04 1.80953182e-03 6.10532647e-04\n","   1.53671179e-04 2.62351148e-03 5.17822309e-06 1.00940757e-03\n","   4.87510022e-03 1.00954225e-04 3.99937970e-04 2.58573244e-04\n","   1.28593529e-04 4.13344496e-05 2.14453181e-03 6.62435268e-05\n","   4.20276301e-05 3.17381346e-05 1.50726657e-04 9.82021391e-01\n","   1.86868747e-05 3.05418871e-05 2.79060262e-03]\n","  [1.50691281e-04 9.95402813e-01 7.88868492e-05 8.56645129e-05\n","   2.85696820e-04 4.08963824e-04 1.55577072e-04 4.34029317e-07\n","   7.87402096e-04 4.06071194e-05 5.86535352e-05 8.09053672e-05\n","   7.49542960e-05 3.63931504e-05 4.02556441e-04 2.07641642e-04\n","   6.13145403e-06 1.58401323e-03 7.30154443e-06 5.73998914e-05\n","   1.27178573e-05 5.86235501e-07 7.38592644e-05]]] (1, 8, 23)\n","Probability of Sentence \" jack fell down and broke his crown \" is: 0.16849966135169817\n","Input Sentence: and jill came tumbling\n","Encoded: [[ 0  2  4 20 21]] (1, 5)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [2.29452595e-01 7.71042425e-04 2.07244128e-01 6.21089479e-03\n","   2.75788549e-03 3.76727176e-03 3.30060546e-04 1.07133924e-03\n","   1.58382900e-04 3.37981194e-01 3.07156728e-03 1.56293936e-05\n","   3.21527448e-04 4.55196743e-04 3.57997604e-04 1.94590300e-01\n","   1.15915907e-04 7.43924081e-03 2.27148092e-04 1.64792477e-03\n","   1.65000628e-03 3.45094537e-04 1.75051609e-05]\n","  [1.44180201e-04 2.97200859e-05 2.50178203e-02 1.56155467e-04\n","   6.66070953e-02 1.29182159e-03 1.30637013e-03 3.73309711e-04\n","   2.49127112e-03 2.46427400e-04 7.32317507e-01 2.45649461e-03\n","   3.38492755e-05 6.31656585e-05 6.14955323e-04 5.15364157e-03\n","   1.48091272e-01 2.18000845e-04 2.06731772e-03 4.69574559e-04\n","   6.65964140e-03 3.77324386e-03 4.17109113e-04]\n","  [6.96673160e-05 1.86189573e-04 6.71603112e-03 3.59111320e-04\n","   2.18801890e-02 6.09395094e-03 7.45329598e-04 9.41721373e-04\n","   5.20009140e-04 1.53429992e-05 1.58985739e-03 9.37522769e-01\n","   9.86599247e-04 3.36476842e-05 3.73376242e-04 7.63294593e-05\n","   1.21832881e-02 1.57400318e-05 8.96238489e-04 1.60928408e-03\n","   1.66810956e-03 2.78746476e-03 2.72965059e-03]\n","  [1.87763202e-04 8.21204681e-04 5.58743486e-05 2.65869021e-04\n","   5.10650761e-06 6.57138473e-04 7.94442929e-03 4.23312740e-04\n","   9.41616745e-05 5.01771690e-04 2.64740834e-06 1.11155293e-03\n","   9.57023621e-01 5.54889906e-03 6.14131250e-06 1.11441186e-04\n","   1.22501106e-05 1.60873719e-02 1.99086629e-04 1.21544610e-04\n","   3.62128322e-03 6.35969976e-04 4.56152996e-03]]] (1, 5, 23)\n","Probability of Sentence \" and jill came tumbling \" is: 1.6305011402822823e-10\n","Input Sentence: g after\n","Encoded: [[ 0 22]] (1, 2)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [6.94179013e-02 1.07726431e-03 4.07846153e-01 2.39828043e-03\n","   2.52330862e-03 5.74643398e-03 3.30407027e-04 1.24822743e-03\n","   3.41093808e-04 1.50605768e-01 6.41800044e-03 1.95798530e-05\n","   3.05352529e-04 3.85607913e-04 4.64379729e-04 3.39428186e-01\n","   1.72107189e-04 6.99509680e-03 2.11133942e-04 1.31834880e-03\n","   2.35791248e-03 3.72964219e-04 1.63613113e-05]]] (1, 2, 23)\n","Probability of Sentence \" g after \" is: 0.00047947358689270914\n","Input Sentence: and jill came tumbling after .\n","Encoded: [[ 0  2  4 20 21 22  1]] (1, 7)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [2.29452595e-01 7.71042425e-04 2.07244128e-01 6.21089479e-03\n","   2.75788549e-03 3.76727176e-03 3.30060546e-04 1.07133924e-03\n","   1.58382900e-04 3.37981194e-01 3.07156728e-03 1.56293936e-05\n","   3.21527448e-04 4.55196743e-04 3.57997604e-04 1.94590300e-01\n","   1.15915907e-04 7.43924081e-03 2.27148092e-04 1.64792477e-03\n","   1.65000628e-03 3.45094537e-04 1.75051609e-05]\n","  [1.44180201e-04 2.97200859e-05 2.50178203e-02 1.56155467e-04\n","   6.66070953e-02 1.29182159e-03 1.30637013e-03 3.73309711e-04\n","   2.49127112e-03 2.46427400e-04 7.32317507e-01 2.45649461e-03\n","   3.38492755e-05 6.31656585e-05 6.14955323e-04 5.15364157e-03\n","   1.48091272e-01 2.18000845e-04 2.06731772e-03 4.69574559e-04\n","   6.65964140e-03 3.77324386e-03 4.17109113e-04]\n","  [6.96673160e-05 1.86189573e-04 6.71603112e-03 3.59111320e-04\n","   2.18801890e-02 6.09395094e-03 7.45329598e-04 9.41721373e-04\n","   5.20009140e-04 1.53429992e-05 1.58985739e-03 9.37522769e-01\n","   9.86599247e-04 3.36476842e-05 3.73376242e-04 7.63294593e-05\n","   1.21832881e-02 1.57400318e-05 8.96238489e-04 1.60928408e-03\n","   1.66810956e-03 2.78746476e-03 2.72965059e-03]\n","  [1.87763202e-04 8.21204681e-04 5.58743486e-05 2.65869021e-04\n","   5.10650761e-06 6.57138473e-04 7.94442929e-03 4.23312740e-04\n","   9.41616745e-05 5.01771690e-04 2.64740834e-06 1.11155293e-03\n","   9.57023621e-01 5.54889906e-03 6.14131250e-06 1.11441186e-04\n","   1.22501106e-05 1.60873719e-02 1.99086629e-04 1.21544610e-04\n","   3.62128322e-03 6.35969976e-04 4.56152996e-03]\n","  [2.48915050e-04 1.31354536e-04 5.16780619e-06 1.44333389e-04\n","   2.13373596e-06 3.34446509e-06 7.14682450e-04 6.21853955e-03\n","   2.77803978e-04 5.61487570e-04 5.19978457e-05 1.73907142e-06\n","   9.48425673e-04 9.76074278e-01 4.10412485e-03 2.44818657e-04\n","   3.29761824e-05 8.70338059e-04 5.95551357e-03 6.75256233e-05\n","   6.15901154e-05 1.93942408e-03 1.33951835e-03]\n","  [1.11291115e-03 2.95107602e-03 1.10150831e-04 2.00277707e-03\n","   3.27737886e-04 3.31580413e-05 5.05078060e-06 1.60491455e-03\n","   5.78642357e-03 1.84501725e-04 7.03473401e-04 4.59128423e-05\n","   3.22695223e-06 1.94216799e-03 9.70809519e-01 1.36792267e-04\n","   5.36354899e-04 7.81769450e-06 2.31491402e-03 7.87492190e-03\n","   7.97403175e-07 4.33969144e-05 1.46178261e-03]]] (1, 7, 23)\n","Probability of Sentence \" and jill came tumbling after . \" is: 9.769598459305234e-17\n","-------------------------------------------------------------------------------\n","Input Sentence: and jill came walkings after .\n","Encoded: [[ 0  2  4 20 22  1]] (1, 6)\n","Prob: [[[4.91171062e-01 1.64528587e-03 3.18481820e-03 4.77596134e-01\n","   1.84385187e-03 1.66918762e-04 5.76600316e-04 3.61525454e-04\n","   1.02020305e-04 1.59476884e-02 3.55796838e-05 5.00548864e-04\n","   6.50341273e-04 3.00078362e-04 9.02443368e-04 9.53803901e-05\n","   7.49386149e-04 7.85720709e-04 1.57224422e-03 1.15450134e-03\n","   1.18151438e-04 6.03680710e-05 4.79473587e-04]\n","  [2.29452595e-01 7.71042425e-04 2.07244128e-01 6.21089479e-03\n","   2.75788549e-03 3.76727176e-03 3.30060546e-04 1.07133924e-03\n","   1.58382900e-04 3.37981194e-01 3.07156728e-03 1.56293936e-05\n","   3.21527448e-04 4.55196743e-04 3.57997604e-04 1.94590300e-01\n","   1.15915907e-04 7.43924081e-03 2.27148092e-04 1.64792477e-03\n","   1.65000628e-03 3.45094537e-04 1.75051609e-05]\n","  [1.44180201e-04 2.97200859e-05 2.50178203e-02 1.56155467e-04\n","   6.66070953e-02 1.29182159e-03 1.30637013e-03 3.73309711e-04\n","   2.49127112e-03 2.46427400e-04 7.32317507e-01 2.45649461e-03\n","   3.38492755e-05 6.31656585e-05 6.14955323e-04 5.15364157e-03\n","   1.48091272e-01 2.18000845e-04 2.06731772e-03 4.69574559e-04\n","   6.65964140e-03 3.77324386e-03 4.17109113e-04]\n","  [6.96673160e-05 1.86189573e-04 6.71603112e-03 3.59111320e-04\n","   2.18801890e-02 6.09395094e-03 7.45329598e-04 9.41721373e-04\n","   5.20009140e-04 1.53429992e-05 1.58985739e-03 9.37522769e-01\n","   9.86599247e-04 3.36476842e-05 3.73376242e-04 7.63294593e-05\n","   1.21832881e-02 1.57400318e-05 8.96238489e-04 1.60928408e-03\n","   1.66810956e-03 2.78746476e-03 2.72965059e-03]\n","  [1.69597159e-04 8.76413193e-04 5.75578742e-05 2.20771952e-04\n","   7.47116110e-06 9.28196241e-04 1.06516620e-02 5.04544703e-04\n","   1.20877899e-04 4.90650185e-04 3.67464213e-06 1.28939864e-03\n","   9.49698687e-01 6.48115482e-03 7.03656178e-06 1.36937349e-04\n","   1.44144642e-05 1.79934576e-02 2.25792406e-04 1.30204658e-04\n","   3.68834729e-03 8.85884045e-04 5.41739026e-03]\n","  [2.16672153e-04 1.19916025e-04 4.15198565e-06 1.38531104e-04\n","   1.75447201e-06 3.34335800e-06 7.68188562e-04 5.32151805e-03\n","   2.78366468e-04 5.12997212e-04 4.26959268e-05 1.56444855e-06\n","   9.74491122e-04 9.77483630e-01 3.74699524e-03 2.07413366e-04\n","   3.20095023e-05 9.36306198e-04 5.66933351e-03 6.15105164e-05\n","   6.73756731e-05 1.92711921e-03 1.48421142e-03]]] (1, 6, 23)\n","Probability of Sentence \" and jill came walkings after . \" is: 1.3993542948322458e-13\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"functional_2\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">79,184</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m78,500\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │        \u001b[38;5;34m79,184\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">157,684</span> (615.95 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m157,684\u001b[0m (615.95 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">157,684</span> (615.95 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m157,684\u001b[0m (615.95 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["<class 'list'>\n","<class 'list'>\n","<class 'list'>\n","0\n","2\n","2\n","<class 'numpy.ndarray'>\n","<class 'numpy.ndarray'>\n","(784, 100)\n","(100,)\n","<class 'numpy.ndarray'>\n","<class 'numpy.ndarray'>\n","(100, 784)\n","(784,)\n","0.07394874\n","0.0\n","-0.08216681\n","0.0\n","Training Autoencoder1:\n","Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.0124 - loss: 0.0605\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0131 - loss: 0.0211\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0127 - loss: 0.0127\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0141 - loss: 0.0093\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.0131 - loss: 0.0076\n","0.070853844\n","1.6502608\n","-0.18924883\n","-0.10209894\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n","(60000, 100)\n","(100, 50)\n","(50, 100)\n","Training Autoencoder2:\n","Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2108 - loss: 0.0741\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.1672 - loss: 0.0278\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.1808 - loss: 0.0166\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.1956 - loss: 0.0133\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.2031 - loss: 0.0121\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n","(60000, 50)\n","(50, 10)\n","Training Classifier:\n","Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4727 - loss: 1.8507\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8403 - loss: 0.8373\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8634 - loss: 0.6002\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8705 - loss: 0.5112\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8782 - loss: 0.4639\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 0.4680\n","[0.4144992530345917, 0.8884999752044678]\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_5\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m78,500\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m5,050\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m510\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,060</span> (328.36 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m84,060\u001b[0m (328.36 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,060</span> (328.36 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m84,060\u001b[0m (328.36 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(100, 50)\n","(100, 50)\n","Fine tuning:\n","Epoch 1/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9050 - loss: 0.3269\n","Epoch 2/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9519 - loss: 0.1706\n","Epoch 3/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9647 - loss: 0.1217\n","Epoch 4/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9736 - loss: 0.0951\n","Epoch 5/5\n","\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9791 - loss: 0.0736\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7d296053e6e0>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow import keras\n","import numpy\n","from numpy import array\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import SimpleRNN\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","data = \"\"\" Jack and Jill went up the hill .\\n To fetch a pail of water .\\n Jack fell down and broke his crown .\\n And Jill came tumbling after .\"\"\"\n","print(\"Data:\", data, type(data))\n","\n","data_splitted=data.split('\\n') #returns a list of strings\n","print(\"Data_Splitted:\", data_splitted, type(data_splitted))\n","\n","tokenizer=Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\]^_`{|}~')\n","\n","tokenizer.fit_on_texts(data_splitted) #learns a vocabulary\n","print(\"Word Indices:\", tokenizer.word_index) #tokenizer.word_index is a dictionary\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","print(\"Vocab Size:\", vocab_size)\n","\n","sequences=tokenizer.texts_to_sequences(data_splitted) #list of list\n","l=len(sequences)\n","print(\"Sequences:\",sequences, type(sequences), l)\n","\n","X=list()\n","y=list()\n","\n","for i in range(len(sequences)):\n","    X.insert(i,sequences[i][:-1])\n","    y.insert(i,sequences[i])\n","\n","print(\"X=\", X, \"y=\",y, type(X), type(y))\n","\n","maxlen = max([len(sequence) for sequence in X])\n","print(\"Maxlen:\",maxlen)\n","\n","#X=array(X)\n","#y=array(y)\n","#X=numpy.reshape(X,newshape=(l,-1))\n","#y=numpy.reshape(y,newshape=(l,-1))\n","\n","X=pad_sequences(X,maxlen=maxlen+1,padding='pre') # +1 to have 0 as the first input\n","print(\"X:\",X, type(X), X.shape)\n","y=pad_sequences(y,maxlen=maxlen+1,padding='pre')\n","print(\"y:\",y, type(y), y.shape)\n","\n","#z=numpy.\n","#for i in range(l):\n","#  z[i]=to_categorical(y[i],num_classes=vocab_size)\n","\n","#print(\"X=\", X, \"y=\",y)\n","y=to_categorical(y,num_classes=vocab_size)\n","\n","print(\"X=\", X, \"y=\",y, type(X), type(y), X.shape, y.shape)\n","model=Sequential()\n","model.add(Embedding(input_dim=vocab_size, output_dim=10))\n","model.add(SimpleRNN(units=50, return_sequences=True))\n","model.add(Dense(units=vocab_size,activation='softmax'))\n","model.summary()\n","\n","model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n","\n","model.fit(X, y, epochs=200, verbose=0)\n","\n","#sample_seq_wo_seed(model, tokenizer, 10, 23)\n","\n","\n","\n","def prob_of_input_sentence(model, tokenizer, sentence):\n","    print(\"Input Sentence:\", sentence)\n","    encoded=tokenizer.texts_to_sequences([sentence])[0]\n","    encoded.insert(0,0)\n","    encoded=array(encoded)\n","    encoded=numpy.reshape(encoded,newshape=(1,-1))\n","    print(\"Encoded:\", encoded, encoded.shape)\n","    prob=model.predict(encoded, verbose=0)\n","    print(\"Prob:\", prob, prob.shape)\n","    probability=1\n","    for i in range(prob.shape[1]-1):\n","        probability = probability * prob[0,i,encoded[0,i+1]]\n","    print(\"Probability of Sentence\", \"\\\"\", sentence, \"\\\"\", \"is:\", probability)\n","\n","print(\"-------------------Probability of Input Sentence------------------------------\")\n","prob_of_input_sentence(model, tokenizer, \"Jack and Jill Went\")\n","prob_of_input_sentence(model, tokenizer, \"and Jill Went up\")\n","prob_of_input_sentence(model, tokenizer, \"went up the hill\")\n","prob_of_input_sentence(model, tokenizer, \"jack and Jill Went up the hill .\")\n","prob_of_input_sentence(model, tokenizer, \"to fetch a pail\")\n","prob_of_input_sentence(model, tokenizer, \"fetch a pail\")\n","prob_of_input_sentence(model, tokenizer, \"to fetch a pail of water\")\n","prob_of_input_sentence(model, tokenizer, \"jack fell down and\")\n","prob_of_input_sentence(model, tokenizer, \"jack fell down and broke\")\n","prob_of_input_sentence(model, tokenizer, \"fell down and broke\")\n","prob_of_input_sentence(model, tokenizer, \"jack fell down and broke his crown\")\n","prob_of_input_sentence(model, tokenizer, \"and jill came tumbling\")\n","prob_of_input_sentence(model, tokenizer, \"g after\")\n","prob_of_input_sentence(model, tokenizer, \"and jill came tumbling after .\")\n","print(\"-------------------------------------------------------------------------------\")\n","\n","prob_of_input_sentence(model, tokenizer, \"and jill came walkings after .\")\n","\n","from tensorflow import keras\n","from keras.layers import Dense, Input\n","from keras.models import Model, Sequential\n","from keras.datasets import mnist\n","import numpy as np\n","from tensorflow.keras.utils import to_categorical\n","#from tensorflow.keras.utils import to_categorical\n","\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = np.reshape(x_train, newshape=(60000, 784)).astype('float32')\n","x_test = np.reshape(x_test, newshape=(10000, 784)).astype('float32')\n","y_train=to_categorical(y_train,num_classes=10)\n","y_test=to_categorical(y_test,num_classes=10)\n","x_train=x_train/255\n","x_test=x_test/255\n","\n","\n","\n","input_main=Input(shape=(784,))\n","h1=Dense(units=100, activation='sigmoid')(input_main)\n","o1=Dense(units=784, activation='sigmoid')(h1)\n","autoencoder1=Model(inputs=input_main, outputs=o1)\n","autoencoder1.summary()\n","autoencoder1.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n","\n","print(type(autoencoder1.layers[0].get_weights()))\n","print(type(autoencoder1.layers[1].get_weights()))\n","print(type(autoencoder1.layers[2].get_weights()))\n","\n","print(len(autoencoder1.layers[0].get_weights()))\n","print(len(autoencoder1.layers[1].get_weights()))\n","print(len(autoencoder1.layers[2].get_weights()))\n","\n","print(type(autoencoder1.layers[1].get_weights()[0]))\n","print(type(autoencoder1.layers[1].get_weights()[1]))\n","print(autoencoder1.layers[1].get_weights()[0].shape)\n","print(autoencoder1.layers[1].get_weights()[1].shape)\n","\n","print(type(autoencoder1.layers[2].get_weights()[0]))\n","print(type(autoencoder1.layers[2].get_weights()[1]))\n","print(autoencoder1.layers[2].get_weights()[0].shape)\n","print(autoencoder1.layers[2].get_weights()[1].shape)\n","\n","print(autoencoder1.layers[1].get_weights()[0][50,50])\n","print(autoencoder1.layers[1].get_weights()[1][10])\n","print(autoencoder1.layers[2].get_weights()[0][50,50])\n","print(autoencoder1.layers[2].get_weights()[1][10])\n","\n","print(\"Training Autoencoder1:\")\n","autoencoder1.fit(x_train,x_train,epochs=5)\n","\n","print(autoencoder1.layers[1].get_weights()[0][50,50])\n","print(autoencoder1.layers[1].get_weights()[1][10])\n","print(autoencoder1.layers[2].get_weights()[0][50,50])\n","print(autoencoder1.layers[2].get_weights()[1][10])\n","\n","autoencoder1_hidden_output=autoencoder1.layers[1].output\n","trimmed_autoencoder1=Model(inputs=input_main, outputs=autoencoder1_hidden_output)\n","x_train_ae2=trimmed_autoencoder1.predict(x_train)\n","x_test_ae2=trimmed_autoencoder1.predict(x_test)\n","print(x_train_ae2.shape)\n","\n","\n","inputs_ae2=Input(shape=(100,))\n","h2=Dense(units=50, activation='sigmoid')(inputs_ae2)\n","o2=Dense(units=100, activation='sigmoid')(h2)\n","autoencoder2=Model(inputs=inputs_ae2, outputs=o2)\n","autoencoder2.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n","print(autoencoder2.layers[1].get_weights()[0].shape)\n","print(autoencoder2.layers[2].get_weights()[0].shape)\n","print(\"Training Autoencoder2:\")\n","autoencoder2.fit(x_train_ae2, x_train_ae2, epochs=5)\n","\n","autoencoder2_hidden_output=autoencoder2.layers[1].output\n","trimmed_autoencoder2=Model(inputs=inputs_ae2, outputs=autoencoder2_hidden_output)\n","x_train_clf=trimmed_autoencoder2.predict(x_train_ae2)\n","x_test_clf=trimmed_autoencoder2.predict(x_test_ae2)\n","print(x_train_clf.shape)\n","\n","inputs_clf=Input(shape=(50,))\n","f_output=Dense(units=10, activation='softmax')(inputs_clf)\n","clf=Model(inputs=inputs_clf, outputs=f_output)\n","clf.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(clf.layers[1].get_weights()[0].shape)\n","print(\"Training Classifier:\")\n","clf.fit(x_train_clf, y_train, epochs=5)\n","\n","\n","print(clf.evaluate(x_test_clf,y_test))\n","\n","\n","new_model=Sequential()\n","new_model.add(autoencoder1.layers[0])\n","new_model.add(autoencoder1.layers[1])\n","new_model.add(autoencoder2.layers[1])\n","new_model.add(clf.layers[-1])\n","new_model.summary()\n","new_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n","print(new_model.layers[1].get_weights()[0].shape)\n","print(autoencoder2.layers[1].get_weights()[0].shape)\n","print(\"Fine tuning:\")\n","new_model.fit(x_train, y_train, epochs=5)\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Review"]},{"cell_type":"markdown","metadata":{},"source":["The model is trained on a small dataset consisting of sentences split into sequences of words. The code preprocesses the data, tokenizes the words, and converts them into sequences. Then, it builds an RNN model with an embedding layer followed by a SimpleRNN layer and a Dense layer. The model is trained to predict the next word in a sequence given the previous words. Finally, the trained model is used to calculate the probability of input sentences."]},{"cell_type":"markdown","metadata":{},"source":["Deep Autoencoder and Classifier Integration: This part involves building a deep autoencoder followed by a classifier. The autoencoder is designed to compress the input data into a lower-dimensional representation, and the classifier is trained to classify the compressed representations into different classes. The code loads the MNIST dataset, preprocesses it, and constructs two autoencoder models: autoencoder1 and autoencoder2. These autoencoders are then used to extract features, and a classifier is trained on these features to classify the MNIST digits."]},{"cell_type":"markdown","metadata":{},"source":["Additionally, the code combines the two parts by fine-tuning the integrated model using both the autoencoder and the classifier. The final model is trained on the MNIST dataset to perform classification after feature extraction using the autoencoders."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
